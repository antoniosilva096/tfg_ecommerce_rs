Creación de procesos etl.

- Crear directorio

- Crear entorno virtual.

- Instalar pandas y dataset de hugging face

- Cargar el dataset desde la api de hugging face

- Obtener las categorias disponibles

- Obtener y analizar los datos disponibles

- Elegir un subconjunto con el que trabajaremos para hacer el dataset más manejable.


- Utilizar únicamente los datos correspondientes a las categorías:

	Electronics
	Computers_and_Accessories
	Cell_Phones_and_Accessories
	Office_Products
	
	
- Seleccionar las métricas necesarias:
	Para productos se utilizarán los metadatos (raw_meta_) que incluyen nombre, precio, marca, etc.
	Para reseñas se emplearán las opiniones (raw_review_) que contienen texto, estrellas, fecha, entre otros.
	Se podrá incorporar información de los otros subconjuntos (por ejemplo, 0core_rating_only_ o 0core_timestamp_) para análisis de tendencias o comportamiento, según los objetivos del análisis.

- Generar un CSV limpio y manejable con pandas:
	Reducir el tamaño original mediante filtrado (por fechas, calidad de datos, y muestreo ) y exportar el resultado en un archivo que luego se utilizará para
	alimentar el sistema recomendador en Django.
	
	
*Explicación Paso a Paso:

- Importaciones y Definición de Funciones
	from datasets import load_dataset: Para cargar los datos desde Hugging Face.
	import pandas as pd: Para manipular DataFrames.
	load_and_filter_data: Función que cargará tanto los metadatos como las reseñas de la categoría que se le pase como parámetro, y devolverá dos DataFrames filtrados.
	
	
- Función main()
	Carga los datos de cada categoría relevante (Electronics, Computers_and_Accessories, Cell_Phones_and_Accessories, Office_Products).
	Concatena (une) los DataFrames de metadatos y de reseñas de todas las categorías.
	(Opcional) Realiza un muestreo si el dataset queda demasiado grande.
	Exporta los DataFrames resultantes a archivos CSV.

- Función load_and_filter_data(category: str)

	Construye la cadena para cargar el subset de metadatos y reseñas (por ejemplo, raw_meta_Electronics y raw_review_Electronics).
	Convierte los datos en DataFrames de Pandas.
	Selecciona las columnas relevantes (por ejemplo, asin, product_title, price, brand para metadatos y asin, reviewer_id, rating, review_body, review_date para reseñas).
	Realiza conversiones de tipos (por ejemplo, price a numérico y review_date a fecha).
	Elimina filas con datos críticos faltantes (por ejemplo, reseñas sin asin, rating o review_date).
	(Opcional) Aplica un filtro por fecha, si quieres descartar reseñas muy antiguas o solo quedarte con las más recientes.
	
	
	
* Al implementar los algoritmos de etl nos hemos quedado sin espacio en disco. para solventarlo entrenaremos al modelo solo con la parte de "electronics"  que es la más pesada a ver si funciona el proximo dia






